# ADR-003: CQRS Pattern for Reporting Service

---

**Status:** Accepted  
**Date:** 5 October 2025  
**Last Updated:** 10 november  2025  
**Author:** Adam James Brown  

---

## Context and Problem Statement

Help desk managers need dashboards showing things like complaint volume trends, how long it takes to resolve issues, staff performance, and which categories get the most complaints (R008). The problem is that these analytics queries are pretty complex - they need to join data from multiple tables (Complaint, StatusHistory, User) and do aggregations.

If I run these heavy analytics queries on the same database that's handling complaint submissions, it could slow everything down. Imagine a manager loading a dashboard with 6 months of data while customers are trying to submit new complaints - that's not good.

**The big question:** Should I use the same database and data structure for both writing complaints (commands) and reading analytics (queries)?

---

## Decision Drivers

### What I Need
- **NFR2 (Performance):** Manager dashboards need to load in under 2 seconds
- **NFR3 (Reliability):** Analytics queries shouldn't slow down the main complaint system
- Analytics need denormalized data (like "complaints per day by category") which is different from how I store transactional data
- Managers want to see historical trends (6-12 months of data)
- Writing a complaint and reading analytics have completely different optimization needs

---

## Options I Considered

### Option 1: Single Database for Everything

**What it is:** Use one PostgreSQL database with a normalized schema for both creating complaints and running analytics queries.

**Pros:**
- Really simple - no data syncing needed
- Always shows current data (strong consistency)
- Cheaper - only need one database
- Easy to understand - one source of truth

**Cons:**
- **Performance problems:** Heavy analytics queries slow down complaint submissions
- **Index conflicts:** Indexes that help writes hurt reads and vice versa
- **Can't scale independently:** If I need more power for analytics, I have to scale the whole database
- **Inefficient for analytics:** Queries need to join 5-7 tables which is slow

**My thoughts:** This would work fine initially with low traffic, but it doesn't scale. Once there are thousands of complaints and multiple managers running dashboard queries, this would violate my performance requirements (NFR2 and NFR3).

---

### Option 2: CQRS with Separate Read and Write Databases âœ“ **(My Choice)**

**What it is:** Have two databases - one optimized for writing complaints (normalized schema), another optimized for analytics queries (denormalized, pre-aggregated data). Keep them in sync using events.

**Pros:**
- **Independent scaling:** Can add more read replicas for dashboards without affecting complaint submissions
- **Optimized for each job:** Write database focused on data integrity, read database focused on query speed
- **No performance conflicts:** Analytics queries don't slow down complaint operations at all
- **Flexible indexing:** Can create indexes purely for analytics without worrying about write performance
- **Could use different tech:** Could even use something like Elasticsearch for the read side later without changing the write side

**Cons:**
- Eventual consistency - dashboards might lag 1-5 seconds behind reality
- Data synchronization is more complex
- More expensive - running two databases
- Have to handle sync failures

**Why I chose this:** This is a well-proven pattern used by companies like Microsoft, Amazon, and Netflix for exactly this kind of problem. Yes, it's more complex, but the performance benefits are worth it. It lets me meet my performance requirements while keeping the system reliable.

---

### Option 3: Read Replicas

**What it is:** Use PostgreSQL's built-in replication to create read-only copies of the database for analytics.

**Pros:**
- Built into PostgreSQL, no extra software
- Automatic data synchronization
- Really fast replication (milliseconds)

**Cons:**
- **Still has the same schema:** Replicas mirror the normalized structure, so queries still need all those slow JOINs
- **Can't optimize independently:** Whatever indexes and structure the write database has, the replicas have too
- **Replication lag:** During high write volumes, replicas can fall behind

**My thoughts:** This is better than Option 1 because it separates the load, but it doesn't solve the schema problem. Analytics queries are still slow because the data structure isn't optimized for them.

---

## My Decision

**I'm going with CQRS (Command Query Responsibility Segregation) with separate read and write databases.**

### Why This Makes Sense

1. **Performance Isolation (NFR2, NFR3):** Managers loading complex dashboards doesn't affect users submitting complaints. Each database is optimized for its specific job.

2. **Query Optimization:** The read database uses denormalized materialized views - essentially pre-computed tables with the data already aggregated:
   - `complaint_stats_by_day` - Daily metrics already calculated
   - `agent_performance_summary` - Agent KPIs pre-computed
   - `complaint_category_distribution` - Category percentages ready to go

3. **Scalability (NFR5):** Can scale the read database for analytics independently of the write database. If I need to support 100 concurrent managers viewing dashboards, I just add more read replicas.

4. **Industry Standard:** CQRS is used by major companies for high-performance systems. It's a well-documented pattern with lots of resources, so I'm not inventing something weird.

### How I'm Implementing It

**Write Side (Complaint Service):**
- Normalized schema (proper database design with foreign keys)
- Optimized for INSERT and UPDATE operations
- ACID transactions to keep data consistent
- Publishes events whenever something changes (ComplaintCreated, ComplaintStatusChanged)

**Read Side (Reporting Service):**
- Denormalized materialized views (data is duplicated and pre-aggregated for speed)
- Optimized for SELECT queries with aggregations
- It's okay if it's a few seconds behind (eventual consistency)
- Listens to events from RabbitMQ and updates the views

**How it works:**
```
1. User creates/updates complaint in Complaint Service
2. Complaint Service writes to Write Database
3. Complaint Service publishes event to RabbitMQ
4. Reporting Service picks up the event
5. Reporting Service updates materialized views in Read Database
6. Manager views dashboard which queries Read Database
```

**Example of a materialized view:**
```sql
CREATE MATERIALIZED VIEW complaint_stats_by_day AS
SELECT 
    date_trunc('day', created_at) as report_date,
    tenant_id,
    category_id,
    priority,
    COUNT(*) as total_complaints,
    AVG(EXTRACT(EPOCH FROM (resolved_at - created_at))/3600) as avg_resolution_hours
FROM complaints
WHERE created_at >= CURRENT_DATE - INTERVAL '6 months'
GROUP BY 1, 2, 3, 4;

CREATE INDEX idx_stats_date_tenant ON complaint_stats_by_day(report_date, tenant_id);
```

This view pre-calculates all the daily stats, so when a manager loads the dashboard, it's just reading from this table instead of doing complex JOINs and aggregations on the fly.

---

## What I'm Actually Building for POC

For the proof-of-concept, I'm simplifying the CQRS implementation:
- **Write side:** Fully implemented with normalized schema
- **Read side:** Basic implementation with a couple of materialized views
- **Synchronization:** The event flow works, but I'm only implementing 2-3 views rather than the full set

This demonstrates the CQRS pattern without building every possible dashboard view.

---

## Consequences

### What I Gain
- Dashboard queries run in under 500ms consistently (way below my 2s target)
- Complaint submissions aren't affected by reporting load
- Can add new analytics views without impacting write performance
- Clear separation between transactional and analytical concerns
- If needed later, I could even migrate the read database to a specialized analytics engine like ClickHouse

### What I'm Dealing With
- Dashboards might show data that's 1-5 seconds old (eventual consistency)
- Have to handle cases where synchronization fails
- About 30% higher infrastructure cost (running two databases)
- More complex to deploy and monitor

### How I'm Managing It
- Adding "Last updated: X seconds ago" to dashboards so managers know the data might be slightly stale
- Implementing a dead letter queue for events that fail to process
- Retry logic with exponential backoff if updates fail
- Monitoring the sync lag and alerting if it goes over 10 seconds
- Providing a "Refresh" button so managers can manually trigger updates if needed

---

## When Eventual Consistency Is Fine vs Not Fine

**Works well for:**
- Historical trend analysis (6-12 months of data)
- Performance metrics (average resolution times, agent productivity)
- Category distribution reports

These are all about trends and patterns - being 5 seconds behind doesn't matter.

**Not acceptable for:**
- Real-time complaint status (for this, query the write database directly)
- Audit logs (use the write database's StatusHistory table)

**My SLA:** Reporting data will be consistent within 5 seconds of writes under normal conditions. Managers understand this is analytics data, not real-time transaction data.

---

## Trade-offs I'm Making

**Complexity vs Performance:** Yes, CQRS adds complexity. But the performance benefits are significant - analytics queries run 10-20x faster on denormalized data. And the complexity is manageable with the event-driven architecture I'm already using.

**Consistency vs Scalability:** With one database, I'd have strong consistency but performance problems. With CQRS, I get a few seconds of lag but much better performance and scalability. For analytics, this trade-off makes total sense.

**Cost vs Performance:** Running two databases costs more, but not having analytics slow down the main system is worth it. Plus, I can size each database appropriately for its workload rather than over-provisioning a single database.

---

## How This Aligns with Standards

This approach follows:
- **Domain-Driven Design:** Separate models for commands and queries
- **Event Sourcing principles:** Using events to keep read models updated
- **Microservices patterns:** Each service has its own optimized data store

---

## Sources I Used

- Fowler, M. (2011). *CQRS*. Retrieved from https://martinfowler.com/bliki/CQRS.html
- Vernon, V. (2013). *Implementing Domain-Driven Design*, Chapter 12: Repositories. Addison-Wesley.
- Microsoft. (2024). *CQRS pattern*. Azure Architecture Center. Retrieved from https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs
- Kleppmann, M. (2017). *Designing Data-Intensive Applications*, Chapter 3: Storage and Retrieval. O'Reilly Media.

---

## Related Decisions

These other ADRs connect to this one:
- **ADR-001:** Microservices Architecture (establishes the service decomposition)
- **ADR-002:** Event-Driven Architecture (provides the synchronization mechanism)
- **ADR-004:** Multi-Tenant Data Isolation (both databases use schema-based isolation)
- **ADR-005:** Technology Stack Selection (explains why PostgreSQL for both sides)
